{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project 3 Week 2\n",
    "\n",
    "Last week we looked at data analysis of big data samples using a *linear cuts* based method. This week we will look at **Neural Networks** which fall into a catagory of methods known as *multivariate analysis* or **MVA**.\n",
    "\n",
    "An **artifical neural network** or **ANN** attempts to simulate part of the way the brain is understood to operate with individual *neurons* being represented by *nodes* in a network. The *nodes* are capable of communicating with other *nodes* in the network just as in the same way as *synapses* work in the brain. The reinforcement or suppression of communication between *neurons* (which is done in the brain via chemical or electrical excitors or inhibitors) is simulated within the **ANN** via weightings between the *neurons*.\n",
    "\n",
    "In this project we are going to use a common type of **ANN**, one known as a **Multilayer Perceptron** or **MLP** with **backpropagation**. **MLPs** are often used in problems of *classification*.\n",
    "\n",
    "For this lecture we will refer to 3 different web sites which give good introductory level information on the use of **ANNs** in general and **MLPs** in particular in `Python`, they are:\n",
    "\n",
    "* Implementing a neural network from scratch ([website](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/) | [iPython version (on Github)](https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb) | [Source code (on Github)](https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb))\n",
    "* How to build a multi-layered neural network in Python ([website](https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a) | [Source code (on Github)](https://github.com/miloharper/multi-layer-neural-network/blob/master/main.py))\n",
    "* A Neural Network in 11 lines of Python ([website](http://iamtrask.github.io/2015/07/12/basic-python-network/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## What to ANNs do?\n",
    "\n",
    "We start with the description of **ANNs** in [Implementing a neural network from scratch](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/) which describes how **neural networks** are powerful classification tools.\n",
    "\n",
    "In this example there are 2 classes of data ('male' and 'female' in the example), represented by *blue* and *red* dots. Each class of data broadly describes a crescent in 2-dimensional xy parameter space. \n",
    "\n",
    "<img src=\"2MoonData.png\" width=\"500\">\n",
    "\n",
    "Here clearly a *linear cuts based* analysis would provide some level of discrimination. However a **neural network** is capable of effectively using *decision boundaries* that are curves rather than straight lines to separate the data. In the case of *n-dimensional* data this can be thought of as *decision boundaries* comprising one or more *hyperplanes* or *hypersurfaces*. This is done by using an **MLP**, which has the following generic structure:\n",
    "\n",
    "<img src=\"MLP.png\" width=\"500\">\n",
    "\n",
    "In this example the **ANN** has an *input layer* with *2 nodes*, one *hidden layer* with *4 nodes* and an *output layer* with *2 nodes*. In general, for simple event classification problems involving just 2 classes only 1 output layer is needed. Some **ANNs** have more than one *hidden layer*, we shall restrict ourselves to a single *hidden layer* in this project.\n",
    "\n",
    "With the **ANN** coded up and applied to the data (we will look at this in more detail below) it can seen that the **ANN** is capable of identifying rather complex *decision boundaries* in order to separate the data.\n",
    "\n",
    "<img src=\"HiddenLayers.png\" width=\"500\">\n",
    "\n",
    "One thing that is important here is the role of the nodes in the *hidden layer*, it can seen, from the diagram above, that each **node** in the *hidden layer* consitutes a *weighted sum* of **all** of the inputs. The **nodes** therefore can represent *features* in the n-dimensional space being used to describe the data that couldn't easily be identified otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Implementing an MLP\n",
    "\n",
    "Whilst the [\"Implementing a neural network from scratch\" website](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/) above does have a well-described implementation of an **ANN** there is another nice line-by-line discussion of the implementation of a simple, 2-layer network at the [\"A Neural Network in 11 lines of Python\" website](http://iamtrask.github.io/2015/07/12/basic-python-network/) which we will look at more closely now. **Note** that we will be using this as a learning tool for the coding of **MLPs** in `Python`, in practise 2-layer networks have very limited application. \n",
    "\n",
    "(Go to [\"A Neural Network in 11 lines of Python\" website](http://iamtrask.github.io/2015/07/12/basic-python-network/))\n",
    "\n",
    "After studying this example you should have a good understanding of the key features of an **MLP** including\n",
    "\n",
    "* non-linear weighting with the **sigmoid** function\n",
    "* backpropagation\n",
    "* hidden layer\n",
    "\n",
    "Using the **MLP** requires 2 distinct operations, first is the *training cycle* where samples of the event classes with known outputs are used to optimise the weights between the different **nodes** in the network. Once trained, the network can then be used to classify new (unseen) data. A simple example of this is illustrated in the **XOR** example below. In general however the process for training, optimising and using an **MLP** is as follows\n",
    "\n",
    "* Define the network parameters (number of nodes per layer, the number of nodes in the input layer is dictated by the number of variables being used to describe your 2 classes of data)\n",
    "* Use *samples* of both classes of data (assuming a 2 data class problem) to train the network. This is an *iterative process* requiring many loops over all the *training data* during which time the weights between the nodes in the **MLP** are optimised. It is possible to monitor how well/quickly the network is being trained by a suitable **metric** such as the mean of the sum of the differences between the required and actual outputs.\n",
    "* Once trained then it is necessary to decide where to place the cut on the network output in order to separate the 2 classes of events. This involves a process similar to that used last week for the linear cuts analysis.\n",
    "* Once the cut level on the network output is determined then sets of (real) data for both classes of events can be passed through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## An example of MLP usage - the XOR problem\n",
    "\n",
    "This problem is well-described on the [\"How to build a multi-layered neural network in Python\" website](https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a). Basically this is a simple problem involving 2 classes of binary data with 3 inputs as described below:\n",
    "\n",
    "<img src=\"XOR.png\" width=\"500\">\n",
    "\n",
    "Here a 3 layer **MLP** is used with a single *hidden layer*. The `Python` code below uses this example but code from the previous ([\"A Neural Network in 11 lines of Python\" website](http://iamtrask.github.io/2015/07/12/basic-python-network/)) site to train the **MLP**.\n",
    "\n",
    "After training the data the network is used to classify a further (unseen) instance.\n",
    "\n",
    "In the following I have modified the original code and renamed their layers 0, 1 and 2 as layers 1, 2 and 3 which I think is more logical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance from required output:0.492843600499\n",
      "Variance from required output:0.014090369407\n",
      "Variance from required output:0.00958596368572\n",
      "Variance from required output:0.00771782642438\n",
      "Variance from required output:0.0066334832453\n",
      "Variance from required output:0.00590454431452\n",
      "Layer 1 weights :\n",
      " [[ 0.3122465   4.57704063 -6.15329916 -8.75834924]\n",
      " [ 0.19676933 -8.74975548 -6.1638187   4.40720501]\n",
      " [-0.03327074 -0.58272995  0.08319184 -0.39787635]]\n",
      "Layer 2 weights :\n",
      " [[ -8.18850925]\n",
      " [ 10.13210706]\n",
      " [-21.33532796]\n",
      " [  9.90935111]]\n",
      "Output for New Input: [[ 0.0078876]]\n"
     ]
    }
   ],
   "source": [
    "# Combination of code from \n",
    "# https://github.com/miloharper/multi-layer-neural-network/blob/master/main.py\n",
    "# and\n",
    "# http://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\n",
    "import numpy as np\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "nLayer1 = 3\n",
    "nLayer2 = 4\n",
    "nLayer3 = 1\n",
    "\n",
    "# Here's a picture of out input and output:\n",
    "#\n",
    "#  Input  Output\n",
    "# [0,0,1]  [0]\n",
    "# [0,1,1]  [1]\n",
    "# [1,0,1]  [1]\n",
    "# [0,1,0]  [1]\n",
    "# [1,0,0]  [1]\n",
    "# [1,1,1]  [0]\n",
    "# [0,0,0]  [1]\n",
    "#\n",
    "\n",
    "InData = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1], [0, 0, 0]])\n",
    "OutData = np.array([[0], [1], [1], [1], [1], [0], [0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "Layer1Weights = 2*np.random.random((nLayer1,nLayer2)) - 1\n",
    "Layer2Weights= 2*np.random.random((nLayer2,nLayer3)) - 1\n",
    "\n",
    "for j in range(60000):\n",
    "\n",
    "    # Feed forward through layers 1,2,3\n",
    "    Layer1 = InData\n",
    "    Layer2 = nonlin(np.dot(Layer1,Layer1Weights))\n",
    "    Layer3 = nonlin(np.dot(Layer2,Layer2Weights))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    Layer3Error = OutData - Layer3\n",
    "    if (j% 10000) == 0:\n",
    "        print (\"Variance from required output:\" + str(np.mean(np.abs(Layer3Error))))\n",
    "\n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    Layer3Delta = Layer3Error*nonlin(Layer3,deriv=True)\n",
    "    \n",
    "    # how much did each Layer2 value contribute to the Layer3 error (according to the weights)?\n",
    "    Layer2Error = Layer3Delta.dot(Layer2Weights.T)\n",
    "\n",
    "    # in what direction is the target Layer2?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    Layer2Delta = Layer2Error * nonlin(Layer2,deriv=True)\n",
    "    \n",
    "    Layer2Weights += Layer2.T.dot(Layer3Delta)\n",
    "    Layer1Weights += Layer1.T.dot(Layer2Delta)\n",
    "\n",
    "print(\"Layer 1 weights :\\n\",Layer1Weights)\n",
    "print(\"Layer 2 weights :\\n\",Layer2Weights)\n",
    "\n",
    "# Test a new input\n",
    "Layer1New = ([[1,1,0]])\n",
    "Layer2New = nonlin(np.dot(Layer1New,Layer1Weights))\n",
    "Layer3New = nonlin(np.dot(Layer2New,Layer2Weights))\n",
    "print(\"Output for New Input:\",Layer3New)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some things to note here include:\n",
    "\n",
    "* The network continues to improve its performance with the number of training cycles. (**Note**, this is not always the case and is a feature of the fact that this is a relatively simple problem).\n",
    "* If a new, unseen input is presented to the **MLP** it accurately predicts the *output*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Project 3 Week 2 Tasks\n",
    "\n",
    "1. If you've not already done so complete the *linear cuts analysis* of the neutrino data (we will come back to these data next week). Note that you can get a good value for the ($\\epsilon\\times\\rho$) metric with a relatively small number of cuts. Also make sure that where a variable is undefined for a particular cut (value of **-999.9** or **None**) then you do not exclude it from the whole analysis - it should only be excluded from that particular cut.\n",
    "\n",
    "2. Take the **XOR MLP** code above and adapt it to take input from the male/female data in the first example studied today. Train the network with a sample of data then test the network with a different dataset (just use a different random number). Study the efficiency of the **MLP** as a function of the following and document your findings in your `Jupyter` notebook. \n",
    "  * The number of events in the training samples\n",
    "  * The number of training loops\n",
    "  * The number of nodes in the hidden layer\n",
    "  \n",
    "3. Repeat 2. with the **noise** variable set to 0.6\n",
    "\n",
    "4. Next week we will be looking at using an **MLP** with the data we used in week 1. You may want to consider adapting your **MLP** code to accept data from the 2 neutrino training samples in preparation for this.\n",
    "\n",
    "### Extra task for 4th years:\n",
    "\n",
    "The `Python` code examples considered above use the so-called **sigmoid function** for the non-linear weighting. It is also possible to use the hyperbolic tangent (**tanh**) function. Attempt the above with these 2 different *weighting functions* and assess which works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}